{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from googletrans import Translator\n",
    "import time\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "\"\"\"\n",
    "docker-compose -f docker-compose-LocalExecutor.yml up -d\n",
    "docker-compose -f docker-compose-LocalExecutor.yml down\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "ventas = pd.read_csv('./data/olist_closed_deals_dataset.csv') # Demian  check\n",
    "clientes = pd.read_csv('./data/olist_customers_dataset.csv') # Demian   check\n",
    "geoloc = pd.read_csv('./data/olist_geolocation_dataset.csv')# Demian   check\n",
    "lideres = pd.read_csv('./data/olist_marketing_qualified_leads_dataset.csv') # Facundo\n",
    "items = pd.read_csv('./data/olist_order_items_dataset.csv') # Facundo\n",
    "pagos = pd.read_csv('./data/olist_order_payments_dataset.csv') # Facundo\n",
    "reviews = pd.read_csv('./data/olist_order_reviews_dataset.csv') # Leo   check\n",
    "venta = pd.read_csv('./data/olist_orders_dataset.csv') # Leo    check\n",
    "productos = pd.read_csv('./data/olist_products_dataset.csv') # Demian   check\n",
    "vendedores = pd.read_csv('./data/olist_sellers_dataset.csv') # Leo  check\n",
    "produtrans = pd.read_csv('./data/product_category_name_translation.csv') # no se usó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seller_id</th>\n",
       "      <th>seller_zip_code_prefix</th>\n",
       "      <th>seller_city</th>\n",
       "      <th>seller_state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3442f8959a84dea7ee197c632cb2df15</td>\n",
       "      <td>13023</td>\n",
       "      <td>campinas</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d1b65fc7debc3361ea86b5f14c68d2e2</td>\n",
       "      <td>13844</td>\n",
       "      <td>mogi guacu</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ce3ad9de960102d0677a81f5d0bb7b2d</td>\n",
       "      <td>20031</td>\n",
       "      <td>rio de janeiro</td>\n",
       "      <td>RJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c0f3eea2e14555b6faeea3dd58c1b1c3</td>\n",
       "      <td>4195</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51a04a8a6bdcb23deccc82b0b80742cf</td>\n",
       "      <td>12914</td>\n",
       "      <td>braganca paulista</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3090</th>\n",
       "      <td>98dddbc4601dd4443ca174359b237166</td>\n",
       "      <td>87111</td>\n",
       "      <td>sarandi</td>\n",
       "      <td>PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3091</th>\n",
       "      <td>f8201cab383e484733266d1906e2fdfa</td>\n",
       "      <td>88137</td>\n",
       "      <td>palhoca</td>\n",
       "      <td>SC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092</th>\n",
       "      <td>74871d19219c7d518d0090283e03c137</td>\n",
       "      <td>4650</td>\n",
       "      <td>sao paulo</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3093</th>\n",
       "      <td>e603cf3fec55f8697c9059638d6c8eb5</td>\n",
       "      <td>96080</td>\n",
       "      <td>pelotas</td>\n",
       "      <td>RS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3094</th>\n",
       "      <td>9e25199f6ef7e7c347120ff175652c3b</td>\n",
       "      <td>12051</td>\n",
       "      <td>taubate</td>\n",
       "      <td>SP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3095 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             seller_id  seller_zip_code_prefix  \\\n",
       "0     3442f8959a84dea7ee197c632cb2df15                   13023   \n",
       "1     d1b65fc7debc3361ea86b5f14c68d2e2                   13844   \n",
       "2     ce3ad9de960102d0677a81f5d0bb7b2d                   20031   \n",
       "3     c0f3eea2e14555b6faeea3dd58c1b1c3                    4195   \n",
       "4     51a04a8a6bdcb23deccc82b0b80742cf                   12914   \n",
       "...                                ...                     ...   \n",
       "3090  98dddbc4601dd4443ca174359b237166                   87111   \n",
       "3091  f8201cab383e484733266d1906e2fdfa                   88137   \n",
       "3092  74871d19219c7d518d0090283e03c137                    4650   \n",
       "3093  e603cf3fec55f8697c9059638d6c8eb5                   96080   \n",
       "3094  9e25199f6ef7e7c347120ff175652c3b                   12051   \n",
       "\n",
       "            seller_city seller_state  \n",
       "0              campinas           SP  \n",
       "1            mogi guacu           SP  \n",
       "2        rio de janeiro           RJ  \n",
       "3             sao paulo           SP  \n",
       "4     braganca paulista           SP  \n",
       "...                 ...          ...  \n",
       "3090            sarandi           PR  \n",
       "3091            palhoca           SC  \n",
       "3092          sao paulo           SP  \n",
       "3093            pelotas           RS  \n",
       "3094            taubate           SP  \n",
       "\n",
       "[3095 rows x 4 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vendedores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmensual = pd.read_csv('./data/olist_closed_deals_dataset.csv')\\nmensual['average_stock'] = mensual['average_stock'].replace('unknown',np.nan)\\nmensual['declared_monthly_revenue'] = mensual['declared_monthly_revenue'].replace(0.0,np.nan)\\nmensual = mensual[['declared_monthly_revenue','lead_behaviour_profile','lead_type']]\\nmensual.loc[mensual['lead_behaviour_profile'].isna(), 'lead_behaviour_profile'] = 'vacio'\\nmensual.loc[mensual['lead_type'].isna(), 'lead_type'] = 'vacio'\\nmensual = mensual.dropna()\\nmensual = mensual.sort_values(by=['declared_monthly_revenue'])\\nmensual\\n\""
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Limpieza y nomalización del DataFrame close_deals\n",
    "'''\n",
    "\n",
    "# Carga del archivo\n",
    "ventas = pd.read_csv('./data/olist_closed_deals_dataset.csv')\n",
    "\n",
    "# Cambiando la columna de fecha de tipo object a tipo datetime\n",
    "ventas['won_date'] = pd.to_datetime(ventas['won_date'])\n",
    "\n",
    "# Reemplazando los valores 0.0 y 'unknown' a np.nan (en columnas numericas)\n",
    "ventas['average_stock'] = ventas['average_stock'].replace('unknown',np.nan)\n",
    "ventas['declared_monthly_revenue'] = ventas['declared_monthly_revenue'].replace(0.0,np.nan)\n",
    "\n",
    "# Reemplazando los valores NaN a 'Sin Dato' (en columnas tipo object/string)\n",
    "ventas['business_segment'] = ventas['business_segment'].replace(np.nan,'Sin Dato')\n",
    "ventas['lead_type'] = ventas['lead_type'].replace(np.nan,'Sin Dato')\n",
    "ventas['lead_behaviour_profile'] = ventas['lead_behaviour_profile'].replace(np.nan,'Sin Dato')\n",
    "ventas['has_company'] = ventas['has_company'].replace(np.nan,'Sin Dato')\n",
    "ventas['has_gtin'] = ventas['has_gtin'].replace(np.nan,'Sin Dato')\n",
    "ventas['business_type'] = ventas['business_type'].replace(np.nan,'Sin Dato')\n",
    "ventas['average_stock'] = ventas['average_stock'].replace(np.nan,'Sin Dato')\n",
    "\n",
    "# Removiendo valores outlier de la columna de declaracion mensual (ver justificacion en la variable mensual, mas abajo)\n",
    "ventas.loc[ventas['declared_monthly_revenue'] < 1000,'declared_monthly_revenue'] = np.nan\n",
    "ventas.loc[ventas['declared_monthly_revenue'] > 500000,'declared_monthly_revenue'] = np.nan\n",
    "\n",
    "'''\n",
    "mensual = pd.read_csv('./data/olist_closed_deals_dataset.csv')\n",
    "mensual['average_stock'] = mensual['average_stock'].replace('unknown',np.nan)\n",
    "mensual['declared_monthly_revenue'] = mensual['declared_monthly_revenue'].replace(0.0,np.nan)\n",
    "mensual = mensual[['declared_monthly_revenue','lead_behaviour_profile','lead_type']]\n",
    "mensual.loc[mensual['lead_behaviour_profile'].isna(), 'lead_behaviour_profile'] = 'vacio'\n",
    "mensual.loc[mensual['lead_type'].isna(), 'lead_type'] = 'vacio'\n",
    "mensual = mensual.dropna()\n",
    "mensual = mensual.sort_values(by=['declared_monthly_revenue'])\n",
    "mensual\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nCabe destacar que en el customer_city final hay un dato menos, queda normalizado planaltina de goias a planaltina\\nprobar lo siguiente:\\nprint(clientes['customer_city'].unique().shape)\\nprint(relacion['customer_city'].unique().shape)\\nrelacion[relacion['customer_city']=='planaltina de goias']\\nclientes[clientes['customer_unique_id']=='7d3d94b4740895a17760e976797f9f0e']\\nrelacion[relacion['customer_city']=='planaltina']\\nNo habria que aplicar otra corrección\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Limpieza y normalizacion del DataFrame customers\n",
    "'''\n",
    "\n",
    "# Carga del archivo\n",
    "clientes = pd.read_csv('./data/olist_customers_dataset.csv')\n",
    "\n",
    "# Creando una serie con lo customer_unique_id repetidos\n",
    "repetidos = []\n",
    "for i in clientes[clientes['customer_unique_id'].duplicated()]['customer_unique_id'].unique():\n",
    "    if clientes[clientes['customer_unique_id'] == i].shape[0] > 1:\n",
    "        repetidos.append(i)\n",
    "repetidos = pd.Series(repetidos)\n",
    "\n",
    "# Creando una serie con los zip codes repetidos dentro de la serie de customer_unique_id repetidos\n",
    "# en resumen: los registros de clientes con todos los datos duplicados, son 250\n",
    "zip_repe = []\n",
    "for i in repetidos:\n",
    "     if (clientes[clientes['customer_unique_id'] == i]['customer_zip_code_prefix'].unique().shape[0]) == 1:\n",
    "        zip_repe.append(i)\n",
    "zip_repe = pd.Series(zip_repe)\n",
    "\n",
    "# Guardo aparte los clientes duplicados\n",
    "guarda = clientes[clientes['customer_unique_id'].isin(zip_repe)].sort_values(by='customer_zip_code_prefix')\n",
    "\n",
    "# Elimino de la tabla original todos los duplicados\n",
    "clientes = clientes.drop(clientes[clientes['customer_unique_id'].isin(zip_repe)].index)\n",
    "\n",
    "# De la tabla guardada remuevo los duplicados, quedandome con el dato mas antiguo\n",
    "guarda = guarda.drop_duplicates(subset='customer_unique_id',keep='first')\n",
    "\n",
    "# Uno las dos tablas quedando limpia de clientes con domicilio repetido\n",
    "clientes = pd.concat([clientes,guarda])\n",
    "\n",
    "# Reseteo el index\n",
    "clientes.index = range(clientes.shape[0])\n",
    "\n",
    "# Queda solucionar el problema del dataframe ordenes que está relacionado a customer_id's que fueron borrados\n",
    "# Abajo se carga una tabla con las dos columnas para ayudar a hacer el merge\n",
    "relacion = pd.read_csv('./data/olist_customers_dataset.csv')\n",
    "relacion = relacion[['customer_id','customer_unique_id']]\n",
    "'''\n",
    "Cabe destacar que en el customer_city final hay un dato menos, queda normalizado planaltina de goias a planaltina\n",
    "probar lo siguiente:\n",
    "print(clientes['customer_city'].unique().shape)\n",
    "print(relacion['customer_city'].unique().shape)\n",
    "relacion[relacion['customer_city']=='planaltina de goias']\n",
    "clientes[clientes['customer_unique_id']=='7d3d94b4740895a17760e976797f9f0e']\n",
    "relacion[relacion['customer_city']=='planaltina']\n",
    "No habria que aplicar otra corrección\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Limpieza y normalizacion del DataFrame products\n",
    "'''\n",
    "\n",
    "# Carga del archivo\n",
    "productos = pd.read_csv('./data/olist_products_dataset.csv')\n",
    "\n",
    "# Reemplazo del _ por espacio para mejor visualizacion del dato\n",
    "productos['product_category_name'] = productos['product_category_name'].str.replace('_',' ')\n",
    "\n",
    "# Reemplazo de los nan por el string 'Sin registrar'\n",
    "#productos['product_category_name'] = productos['product_category_name'].replace(np.nan,'Sin registrar')\n",
    "\n",
    "# Traduciendo la categoria con google translate\n",
    "prod = pd.DataFrame(productos['product_category_name'].unique())\n",
    "prod = prod.rename(columns={0:'product_category_name'})\n",
    "prod['espanol'] = ''\n",
    "# Traduciendo\n",
    "traductor = Translator(service_urls=['translate.google.com.ar'])\n",
    "for i in range(0,len(prod['product_category_name'])):\n",
    "    traduccion = traductor.translate(prod['product_category_name'][i], src='pt', dest='es')\n",
    "    prod['espanol'][i] = traduccion.text\n",
    "# Uniendo la lista traducida y reacomodando\n",
    "productos = pd.merge(productos, prod, on ='product_category_name', how = 'left')\n",
    "productos = productos[['product_id','product_category_name','espanol','product_name_lenght','product_description_lenght',\n",
    "                    'product_photos_qty','product_weight_g','product_length_cm','product_height_cm','product_width_cm']]\n",
    "\n",
    "# Colocando primera letra mayúscula\n",
    "productos['product_category_name'] = productos['product_category_name'].str.capitalize()\n",
    "productos['espanol'] = productos['espanol'].str.capitalize()\n",
    "\n",
    "# Reemplazando los valores NaN a 'Sin Dato' (en columnas tipo object/string)\n",
    "productos['product_category_name'] = productos['product_category_name'].replace(np.nan,'Sin Dato')\n",
    "productos['espanol'] = productos['espanol'].replace('Yaya','Sin Dato')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Limpieza y normalizacion del DataFrame geolocation\n",
    "'''\n",
    "\n",
    "# Se carga el df original y una variable con los zip unicos\n",
    "geoloc = pd.read_csv('./data/olist_geolocation_dataset.csv')\n",
    "zipuni = geoloc['geolocation_zip_code_prefix'].unique()\n",
    "\n",
    "# Se crea otro df con las mismas columnas (distinto nombre) para cargar los datos limpios\n",
    "geoloc_limpia = pd.DataFrame(columns=['zip','lat','lon','ciudad','estado'])\n",
    "\n",
    "# Hay 1 millon de datos en la tabla oribinal y solo 19 mil zip unicos\n",
    "# Se procede a calcular la media de la ubicacion por cada zip\n",
    "# Se agregan al df vacio. No se pierde ningun zip, ni se repite, lo cual serviría como PK\n",
    "ind = 0\n",
    "for i in range(0,len(zipuni)):\n",
    "    zip = zipuni[i]\n",
    "    filtro = geoloc[geoloc['geolocation_zip_code_prefix'] == zip]\n",
    "    lat = filtro['geolocation_lat'].mean()\n",
    "    lon = filtro['geolocation_lng'].mean()\n",
    "    ciudad = filtro.iloc[0]['geolocation_city']\n",
    "    estado = filtro.iloc[0]['geolocation_state']\n",
    "    agrega = pd.DataFrame({'zip':zip,'lat':lat,'lon':lon,'ciudad':ciudad,'estado':estado},index=[ind])\n",
    "    geoloc_limpia = pd.concat([geoloc_limpia,agrega])\n",
    "    ind+=1\n",
    "\n",
    "# Se detectan 15 outliers de ubicaciones fuera del pais buscando de esta forma\n",
    "#limpia = geoloc_limpia[geoloc_limpia['lat'] > -15]\n",
    "#limpia = limpia[limpia['lon'] > - 30]\n",
    "\n",
    "# En base a los nombres de las ciudades se les asigna su ubicacion correcta\n",
    "# (se corrobora con las coord de otras ciudades del mismo estado)\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 18243,'lat'] = -23.54584\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 18243,'lon'] = -48.30460\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 46560,'lat'] = -12.98470\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 46560,'lon'] = -42.22145\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 57319,'lat'] = -9.73376\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 57319,'lon'] = -36.73074\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 58441,'lat'] = -7.18219\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 58441,'lon'] = -35.98457\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 68275,'lat'] = -1.46698\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 68275,'lon'] = -56.37919\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 68379,'lat'] = -8.31638\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 68379,'lon'] = -55.10078\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 78131,'lat'] = -15.64872\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 78131,'lon'] = -56.14690\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 83252,'lat'] = -25.53323\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 83252,'lon'] = -48.50862\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 95130,'lat'] = -29.24143\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 95130,'lon'] = -51.01992\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 68447,'lat'] = -1.50876\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 68447,'lon'] = -48.61568\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 11990,'lat'] = -25.01060\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 11990,'lon'] = -47.92986\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 15855,'lat'] = -21.17783\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 15855,'lon'] = -49.19197\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 28155,'lat'] = -21.97254 # hay 5 santa maria distintos en RJ\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 28155,'lon'] = -42.00692 # se eligió uno\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 47310,'lat'] = -9.29524\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 47310,'lon'] = -40.81975\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 29654,'lat'] = -19.82352\n",
    "geoloc_limpia.loc[geoloc_limpia['zip'] == 29654,'lon'] = -40.65459"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "x = pd.Series(geoloc_limpia['lat'])\n",
    "y = pd.Series(geoloc_limpia['lon'])\n",
    "\n",
    "mapa = folium.Map(location = [x.mean(),y.mean()], zoom_start=4, control_scale=True)\n",
    "\n",
    "mc = MarkerCluster()\n",
    "for row in geoloc_limpia.values:\n",
    "    mc.add_child(folium.Marker(location=[row[1],row[2]]))\n",
    "mapa.add_child(mc)\n",
    "\n",
    "mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = geoloc_limpia[['lat','lon']]\n",
    "heatmap['heat'] = pd.Series(0.2,index=range(0,19015))\n",
    "\n",
    "mapa = folium.Map(location = [geoloc_limpia['lat'].mean(),geoloc_limpia['lon'].mean()], zoom_start=4, control_scale=True)\n",
    "\n",
    "HeatMap(heatmap,radius=5,blur=5).add_to(mapa)\n",
    "#mapa.save('heatmap.html')\n",
    "mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Limpieza y normalizacion del DataFrame reviews\n",
    "'''\n",
    "\n",
    "# Cargando el archivo\n",
    "reviews = pd.read_csv('./data/olist_order_reviews_dataset.csv')\n",
    "\n",
    "# Reemplazo nulos en comentario por 'Sin Dato'\n",
    "reviews['review_comment_message'] = reviews['review_comment_message'].replace(np.nan,'Sin Dato')\n",
    "reviews['review_comment_title'] = reviews['review_comment_title'].replace(np.nan,'Sin Dato')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Limpieza y normalizacion del DataFrame sellers\n",
    "'''\n",
    "\n",
    "# Cargando el archivo\n",
    "vendedores = pd.read_csv('./data/olist_sellers_dataset.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Limpieza y normalizacion del DataFrame orders\n",
    "'''\n",
    "\n",
    "# Cargando el archivo\n",
    "venta = pd.read_csv('./data/olist_orders_dataset.csv') \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' ingestamos en un dataframe los clientes calificados de marketing a partir \n",
    "de las columnas landing_page_id y origen obtenemos dos tablas con valores unicos para la normalizacion de estos campos'''\n",
    "\n",
    "df10= pd.read_csv('./data/olist_marketing_qualified_leads_dataset.csv')\n",
    "#lideres = pd.read_csv('./data/olist_marketing_qualified_leads_dataset.csv')\n",
    "\n",
    "# Creamos un df para normalizar los id de pagina alfanumericos\n",
    "\n",
    "df20= pd.DataFrame(df10['landing_page_id'].unique())\n",
    "df20.columns =['landing_page_id']\n",
    "df20.columns.name='IdPage'\n",
    "df20['IdPages']=np.arange(1,len(df20)+1)\n",
    "df20.to_csv(\"~/ip_files/olist_landing_page_id_Normalizada.csv\")\n",
    "\n",
    "\n",
    "'''Creo una columna Idlanding para sustituir el campo landing_page_id alfanumerico por un valor numerico'''\n",
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy import fuzz\n",
    "landing = df20.landing_page_id.value_counts().index\n",
    "landing_page_unique = df10.landing_page_id.unique()\n",
    "normalized = []\n",
    "def get_matches(query,choices):\n",
    "    for i in query:\n",
    "        tuple = process.extractOne(i,choices)\n",
    "        normalized.append(tuple[0])\n",
    "    return normalized\n",
    "landing_page_correg = get_matches(landing_page_unique, landing)\n",
    "mydict = {landing_page_unique[i]: df20[df20['landing_page_id']==landing_page_correg[i]]['IdPages'] for i in range(0,495)}\n",
    "df10 = df10.convert_dtypes()\n",
    "df10[\"Idlanding\"] = df10[\"landing_page_id\"].map(mydict)\n",
    "df31 =df10['Idlanding']\n",
    "df32=list(df31.astype(str))\n",
    "df31=np.arange(len(df31))\n",
    "lindice=[]\n",
    "for i in df31 :\n",
    "    cadena=df32[i].split(\"\\n\")\n",
    "    indice=cadena[0].split(\" \")\n",
    "    lindice.append(indice[4])\n",
    "df10['IdLandingN']=lindice\n",
    "df10.drop(['landing_page_id','Idlanding'], axis=1, inplace=True)\n",
    "df10.rename(columns={'IdLandingN': 'landing_page_id'}, inplace=True)\n",
    "\n",
    "'''Creo una columna IdOrigin para sustituir el campo Origin alfanumerico por un valor numerico'''\n",
    "df10['origin'].fillna('unknown',inplace=True)\n",
    "df21=pd.DataFrame(df10['origin'].unique())\n",
    "df21.columns =['Origin']\n",
    "df21.columns.name='IdOrigin'\n",
    "df21['IdPages']=np.arange(1,len(df21)+1)\n",
    "df21.to_csv(\"~/ip_files/olist_IdOrigin_Normalizada.csv\")\n",
    "\n",
    "#comparo y sustituyo\n",
    "Origin = df21.Origin.value_counts().index\n",
    "Origin_unique = df10.origin.unique()\n",
    "normalized = []\n",
    "def get_matches(query,choices):\n",
    "    for i in query:\n",
    "        tuple = process.extractOne(i,choices)\n",
    "        normalized.append(tuple[0])\n",
    "    return normalized\n",
    "Origin_correg = get_matches(Origin_unique, Origin)\n",
    "mydict = {Origin_unique[i]: df21[df21['Origin']==Origin_correg[i]]['IdPages'] for i in range(0,10)}\n",
    "df10 = df10.convert_dtypes()\n",
    "df10[\"IdOrigin\"] = df10[\"origin\"].map(mydict)\n",
    "df33 =df10['IdOrigin']\n",
    "df34=list(df33.astype(str))\n",
    "df33=np.arange(len(df33))\n",
    "lindice=[]\n",
    "for i in df33 :\n",
    "    cadena=df34[i].split(\"\\n\")\n",
    "    indice=cadena[0].split(\" \")\n",
    "    lindice.append(indice[4])\n",
    "df10['IdOriginN']=lindice\n",
    "df10.drop(['origin','IdOrigin'], axis=1, inplace=True)\n",
    "df10.rename(columns={'IdOriginN': 'IdOrigin'}, inplace=True)\n",
    "\n",
    "'''creacion de tabla con valores unicos para normalización'''\n",
    "df35= pd.DataFrame(df10['mql_id'].unique())\n",
    "df35.columns =['mql_id']\n",
    "df35.columns.name='Imql_id'\n",
    "df35['mql_idN']=np.arange(1,len(df35)+1)\n",
    "df35.to_csv(\"~/ip_files/olist_mql_idN_Normalizada.csv\")\n",
    "\n",
    "'''Creo una columna mql_idN para sustituir el campo mql_id alfanumerico por un valor numerico'''\n",
    "df10['mql_idN']=np.arange(1,len(df10)+1)\n",
    "df10.drop(['mql_id'], axis=1, inplace=True)\n",
    "df10.rename(columns={'mql_idN': 'mql_id'}, inplace=True)\n",
    "\n",
    "'''Cambiar el tipo de datos a las columnas'''\n",
    "df10['first_contact_date'] = pd.to_datetime(df10['first_contact_date'])\n",
    "df10['landing_page_id'] = pd.to_numeric(df10['landing_page_id'])\n",
    "df10['IdOrigin'] = pd.to_numeric(df10['IdOrigin'])\n",
    "df10['mql_id'] = pd.to_numeric(df10['mql_id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IdVenta</th>\n",
       "      <th>IdCliente</th>\n",
       "      <th>fecha_compra</th>\n",
       "      <th>fecha_aprobado</th>\n",
       "      <th>fecha_envio</th>\n",
       "      <th>fecha_arribo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53cdb2fc8bc7dce0b6741e2150273451</td>\n",
       "      <td>b0830fb4747a6c6d20dea0b8c802d7ef</td>\n",
       "      <td>2018-07-24 20:41:37</td>\n",
       "      <td>2018-07-26 03:24:27</td>\n",
       "      <td>2018-07-26 14:31:00</td>\n",
       "      <td>2018-08-07 15:27:45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47770eb9100c2d0c44946d9cf07ec65d</td>\n",
       "      <td>41ce2a54c0b03bf3443c3d931a367089</td>\n",
       "      <td>2018-08-08 08:38:49</td>\n",
       "      <td>2018-08-08 08:55:23</td>\n",
       "      <td>2018-08-08 13:50:00</td>\n",
       "      <td>2018-08-17 18:06:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>949d5b44dbf5de918fe9c16f97b45f8a</td>\n",
       "      <td>f88197465ea7920adcdbec7375364d82</td>\n",
       "      <td>2017-11-18 19:28:06</td>\n",
       "      <td>2017-11-18 19:45:59</td>\n",
       "      <td>2017-11-22 13:39:59</td>\n",
       "      <td>2017-12-02 00:28:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ad21c59c0840e6cb83a9ceb5573f8159</td>\n",
       "      <td>8ab97904e6daea8866dbdbc4fb7aad2c</td>\n",
       "      <td>2018-02-13 21:18:39</td>\n",
       "      <td>2018-02-13 22:20:29</td>\n",
       "      <td>2018-02-14 19:46:34</td>\n",
       "      <td>2018-02-16 18:17:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            IdVenta                         IdCliente  \\\n",
       "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "1  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
       "2  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
       "3  949d5b44dbf5de918fe9c16f97b45f8a  f88197465ea7920adcdbec7375364d82   \n",
       "4  ad21c59c0840e6cb83a9ceb5573f8159  8ab97904e6daea8866dbdbc4fb7aad2c   \n",
       "\n",
       "          fecha_compra       fecha_aprobado          fecha_envio  \\\n",
       "0  2017-10-02 10:56:33  2017-10-02 11:07:15  2017-10-04 19:55:00   \n",
       "1  2018-07-24 20:41:37  2018-07-26 03:24:27  2018-07-26 14:31:00   \n",
       "2  2018-08-08 08:38:49  2018-08-08 08:55:23  2018-08-08 13:50:00   \n",
       "3  2017-11-18 19:28:06  2017-11-18 19:45:59  2017-11-22 13:39:59   \n",
       "4  2018-02-13 21:18:39  2018-02-13 22:20:29  2018-02-14 19:46:34   \n",
       "\n",
       "          fecha_arribo  \n",
       "0  2017-10-10 21:25:13  \n",
       "1  2018-08-07 15:27:45  \n",
       "2  2018-08-17 18:06:29  \n",
       "3  2017-12-02 00:28:42  \n",
       "4  2018-02-16 18:17:02  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venta = venta[['order_id','customer_id','order_purchase_timestamp','order_approved_at','order_delivered_carrier_date','order_delivered_customer_date']]\n",
    "venta = venta.rename(columns={'order_id':'IdVenta','customer_id':'IdCliente','order_purchase_timestamp':'fecha_compra','order_approved_at':'fecha_aprobado','order_delivered_carrier_date':'fecha_envio','order_delivered_customer_date':'fecha_arribo'})\n",
    "venta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>shipping_limit_date</th>\n",
       "      <th>price</th>\n",
       "      <th>freight_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00010242fe8c5a6d1ba2dd792cb16214</td>\n",
       "      <td>1</td>\n",
       "      <td>4244733e06e7ecb4970a6e2683c13e61</td>\n",
       "      <td>48436dade18ac8b2bce089ec2a041202</td>\n",
       "      <td>2017-09-19 09:45:35</td>\n",
       "      <td>58.90</td>\n",
       "      <td>13.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00018f77f2f0320c557190d7a144bdd3</td>\n",
       "      <td>1</td>\n",
       "      <td>e5f2d52b802189ee658865ca93d83a8f</td>\n",
       "      <td>dd7ddc04e1b6c2c614352b383efe2d36</td>\n",
       "      <td>2017-05-03 11:05:13</td>\n",
       "      <td>239.90</td>\n",
       "      <td>19.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000229ec398224ef6ca0657da4fc703e</td>\n",
       "      <td>1</td>\n",
       "      <td>c777355d18b72b67abbeef9df44fd0fd</td>\n",
       "      <td>5b51032eddd242adc84c38acab88f23d</td>\n",
       "      <td>2018-01-18 14:48:30</td>\n",
       "      <td>199.00</td>\n",
       "      <td>17.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00024acbcdf0a6daa1e931b038114c75</td>\n",
       "      <td>1</td>\n",
       "      <td>7634da152a4610f1595efa32f14722fc</td>\n",
       "      <td>9d7a1d34a5052409006425275ba1c2b4</td>\n",
       "      <td>2018-08-15 10:10:18</td>\n",
       "      <td>12.99</td>\n",
       "      <td>12.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00042b26cf59d7ce69dfabb4e55b4fd9</td>\n",
       "      <td>1</td>\n",
       "      <td>ac6c3623068f30de03045865e4e10089</td>\n",
       "      <td>df560393f3a51e74553ab94004ba5c87</td>\n",
       "      <td>2017-02-13 13:57:51</td>\n",
       "      <td>199.90</td>\n",
       "      <td>18.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id  order_item_id  \\\n",
       "0  00010242fe8c5a6d1ba2dd792cb16214              1   \n",
       "1  00018f77f2f0320c557190d7a144bdd3              1   \n",
       "2  000229ec398224ef6ca0657da4fc703e              1   \n",
       "3  00024acbcdf0a6daa1e931b038114c75              1   \n",
       "4  00042b26cf59d7ce69dfabb4e55b4fd9              1   \n",
       "\n",
       "                         product_id                         seller_id  \\\n",
       "0  4244733e06e7ecb4970a6e2683c13e61  48436dade18ac8b2bce089ec2a041202   \n",
       "1  e5f2d52b802189ee658865ca93d83a8f  dd7ddc04e1b6c2c614352b383efe2d36   \n",
       "2  c777355d18b72b67abbeef9df44fd0fd  5b51032eddd242adc84c38acab88f23d   \n",
       "3  7634da152a4610f1595efa32f14722fc  9d7a1d34a5052409006425275ba1c2b4   \n",
       "4  ac6c3623068f30de03045865e4e10089  df560393f3a51e74553ab94004ba5c87   \n",
       "\n",
       "   shipping_limit_date   price  freight_value  \n",
       "0  2017-09-19 09:45:35   58.90          13.29  \n",
       "1  2017-05-03 11:05:13  239.90          19.93  \n",
       "2  2018-01-18 14:48:30  199.00          17.87  \n",
       "3  2018-08-15 10:10:18   12.99          12.79  \n",
       "4  2017-02-13 13:57:51  199.90          18.14  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "items.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb005c365038adc55fb7897537444f2e763afd05144f871366f10ce7eef6d0d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
